{"nbformat":4,"nbformat_minor":0,"metadata":{"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3,"colab":{"name":"MedicalImageClassification.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"9Cm60yJ8OumZ","colab_type":"text"},"source":["## ドライブをマウント"]},{"cell_type":"code","metadata":{"id":"odKUREhZs50x","colab_type":"code","outputId":"8c459f9e-a08b-4d6e-a994-697a785eb748","executionInfo":{"status":"ok","timestamp":1586745213090,"user_tz":-540,"elapsed":801,"user":{"displayName":"Shinnosuke Matsuo","photoUrl":"","userId":"04948413199740942872"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8IXBDm4QtyoE","colab_type":"code","outputId":"5bb3a5be-d802-455e-8e5a-8f8bc0f07e7a","executionInfo":{"status":"ok","timestamp":1586745214925,"user_tz":-540,"elapsed":2598,"user":{"displayName":"Shinnosuke Matsuo","photoUrl":"","userId":"04948413199740942872"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["!ls 'drive/My Drive/Colab Notebooks/2_MedicalImageClassification/Dataset'"],"execution_count":5,"outputs":[{"output_type":"stream","text":["test\t     train\t   val\t       X_test.npy   X_val.npy\ty_train.npy\n","test.tar.gz  train.tar.gz  val.tar.gz  X_train.npy  y_test.npy\ty_val.npy\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"c4fVhVtyO6b8","colab_type":"text"},"source":["## tarfileを解凍"]},{"cell_type":"code","metadata":{"id":"8ebbXNf_uY98","colab_type":"code","colab":{}},"source":["# import tarfile\n","\n","# for s in [\"train\", \"val\", \"test\"]:\n","#   print(s+\" start!\")\n","#   with tarfile.open('drive/My Drive/Colab Notebooks/2_MedicalImageClassification/Dataset/'+s+'.tar.gz', 'r:*') as tar:\n","#     tar.extractall('drive/My Drive/Colab Notebooks/2_MedicalImageClassification/Dataset')\n","#   print(s+\" finish!\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kM2dIHwBPA-2","colab_type":"text"},"source":["## 画像をロード"]},{"cell_type":"code","metadata":{"id":"5CEaHJhvunpp","colab_type":"code","colab":{}},"source":["# import glob\n","# from tqdm import tqdm\n","# import cv2\n","# import numpy as np\n","# import matplotlib.pyplot as plt\n","# import time\n","\n","# time.sleep(1)\n","# print(\"\\n---------------------------------\")\n","# print(\"train data loading...\")\n","# time.sleep(1)\n","\n","# X_train_list = []\n","# y_train_list = []\n","# for i in range(2):\n","#   files =glob.glob(\"drive/My Drive/Colab Notebooks/2_MedicalImageClassification/Dataset/train/\"+str(i)+\"/*\")\n","#   for fname in tqdm(files):\n","#     img = cv2.imread(fname, 0)\n","#     X_train_list.append(img)\n","#     y_train_list.append(i)\n","\n","# X_train = np.array(X_train_list)\n","# y_train = np.array(y_train_list)\n","# print(\"\")\n","# print(\"X_train shape: {}\".format(X_train.shape))\n","# print(\"y_train shape: {}\".format(y_train.shape))\n","\n","# time.sleep(1)\n","# print(\"\\n---------------------------------\")\n","# print(\"validation data loading...\")\n","# time.sleep(1)\n","\n","# X_val_list = []\n","# y_val_list = []\n","# for i in range(2):\n","#   files =glob.glob(\"drive/My Drive/Colab Notebooks/2_MedicalImageClassification/Dataset/val/\"+str(i)+\"/*\")\n","#   for fname in tqdm(files):\n","#     img = cv2.imread(fname, 0)\n","#     X_val_list.append(img)\n","#     y_val_list.append(i)\n","\n","# X_val = np.array(X_val_list)\n","# y_val = np.array(y_val_list)\n","# print(\"\")\n","# print(\"X_val shape: {}\".format(X_val.shape))\n","# print(\"y_val shape: {}\".format(y_val.shape))\n","\n","# time.sleep(1)\n","# print(\"\\n---------------------------------\")\n","# print(\"test data loading...\")\n","# time.sleep(1)\n","\n","# X_test_list = []\n","# y_test_list = []\n","# for i in range(2):\n","#   files =glob.glob(\"drive/My Drive/Colab Notebooks/2_MedicalImageClassification/Dataset/test/\"+str(i)+\"/*\")\n","#   for fname in tqdm(files):\n","#     img = cv2.imread(fname, 0)\n","#     X_test_list.append(img)\n","#     y_test_list.append(i)\n","\n","# X_test = np.array(X_test_list)\n","# y_test = np.array(y_test_list)\n","# print(\"\")\n","# print(\"X_test shape: {}\".format(X_test.shape))\n","# print(\"y_test shape: {}\".format(y_test.shape))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QDfdTd1cPJlG","colab_type":"text"},"source":["## ndaarayをnpyで保存"]},{"cell_type":"code","metadata":{"id":"jlyD6P0ZM6NJ","colab_type":"code","colab":{}},"source":["# np.save(\"drive/My Drive/Colab Notebooks/2_MedicalImageClassification/Dataset/X_train\", X_train)\n","# np.save(\"drive/My Drive/Colab Notebooks/2_MedicalImageClassification/Dataset/y_train\", y_train)\n","# np.save(\"drive/My Drive/Colab Notebooks/2_MedicalImageClassification/Dataset/X_val\", X_val)\n","# np.save(\"drive/My Drive/Colab Notebooks/2_MedicalImageClassification/Dataset/y_val\", y_val)\n","# np.save(\"drive/My Drive/Colab Notebooks/2_MedicalImageClassification/Dataset/X_test\", X_test)\n","# np.save(\"drive/My Drive/Colab Notebooks/2_MedicalImageClassification/Dataset/y_test\", y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Axhiz-I4PYsu","colab_type":"text"},"source":["## ndarray形式の画像データをロード"]},{"cell_type":"code","metadata":{"id":"KWETSzT-NW2z","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","X_train = np.load(\"drive/My Drive/Colab Notebooks/2_MedicalImageClassification/Dataset/X_train.npy\")\n","y_train = np.load(\"drive/My Drive/Colab Notebooks/2_MedicalImageClassification/Dataset/y_train.npy\")\n","X_val = np.load(\"drive/My Drive/Colab Notebooks/2_MedicalImageClassification/Dataset/X_val.npy\")\n","y_val = np.load(\"drive/My Drive/Colab Notebooks/2_MedicalImageClassification/Dataset/y_val.npy\")\n","X_test = np.load(\"drive/My Drive/Colab Notebooks/2_MedicalImageClassification/Dataset/X_test.npy\")\n","y_test = np.load(\"drive/My Drive/Colab Notebooks/2_MedicalImageClassification/Dataset/y_test.npy\")\n","\n","\n","\n","from keras.utils.np_utils import to_categorical\n","\n","X_train = X_train[..., None]\n","X_val = X_val[..., None]\n","X_test = X_test[..., None]\n","y_train_oh = to_categorical(y_train)\n","y_val_oh = to_categorical(y_val)\n","y_test_oh = to_categorical(y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BJ9hei0z9U6O","colab_type":"text"},"source":["## 関数定義"]},{"cell_type":"markdown","metadata":{"id":"8uL91KhSzE3z","colab_type":"text"},"source":["### 学習履歴表示"]},{"cell_type":"code","metadata":{"id":"utp_bm1a9hwA","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","\n","def plot_history_loss(hist, model_name):\n","  fig = plt.figure()\n","  plt.plot(hist.history['loss'],label=\"train\")\n","  plt.plot(hist.history['val_loss'],label=\"val\")\n","  plt.title('model loss')\n","  plt.xlabel('epoch')\n","  plt.ylabel('loss')\n","  plt.legend(loc='best')\n","  plt.show()\n","  fig.savefig(\"drive/My Drive/Colab Notebooks/2_MedicalImageClassification/Model/\"+model_name+\"_loss.png\")\n","\n","def plot_history_acc(hist, model_name):\n","  fig = plt.figure()\n","  plt.plot(hist.history['accuracy'],label=\"train\")\n","  plt.plot(hist.history['val_accuracy'],label=\"val\")\n","  plt.title('model accuracy')\n","  plt.xlabel('epoch')\n","  plt.ylabel('accuracy')\n","  plt.legend(loc='best')\n","  plt.ylim([0, 1])\n","  plt.show()\n","  fig.savefig(\"drive/My Drive/Colab Notebooks/2_MedicalImageClassification/Model/\"+model_name+\"_acc.png\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k3GQtTlgzMlL","colab_type":"text"},"source":["### 学習の評価"]},{"cell_type":"code","metadata":{"id":"8G2hEoM1zK4d","colab_type":"code","colab":{}},"source":["def model_evaluate(model, X, y):\n","  score = model.evaluate(X_test, y_test_oh, verbose=1)\n","  print(\"--------------------------------------------\")\n","  print(\"--------------------------------------------\")\n","  print(\"test data accuracy: \", score[1])\n","  print(\"test data loss: \", score[0])\n","  print(\"--------------------------------------------\")\n","  print(\"--------------------------------------------\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cExu7sJnzSoZ","colab_type":"text"},"source":["### モデルの保存"]},{"cell_type":"code","metadata":{"id":"HzJogkgjzXCv","colab_type":"code","colab":{}},"source":["def model_save(model, model_name): \n","  json_string = model.to_json()\n","  with open((\"drive/My Drive/Colab Notebooks/2_MedicalImageClassification/Model/\"+model_name+\".model\"), \"w\") as f:\n","      f.write(json_string)\n","  model.save_weights(\"drive/My Drive/Colab Notebooks/2_MedicalImageClassification/Model/\"+model_name+\"_param.hdf5\")\n","  print(\"Saved \"+model_name+\" model\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oGHyx_8NzaX4","colab_type":"text"},"source":["### モデルの読み込み"]},{"cell_type":"code","metadata":{"id":"ZWv9dqzjzgMn","colab_type":"code","colab":{}},"source":["from keras.models import model_from_json\n","\n","def model_load(model_name):\n","  with open((\"drive/My Drive/Colab Notebooks/2_MedicalImageClassification/Model/\"+model_name+\".model\"), \"r\") as f:\n","      json_string = f.read()\n","  model = model_from_json(json_string)\n","  model.load_weights(\"drive/My Drive/Colab Notebooks/2_MedicalImageClassification/Model/\"+model_name+\"_param.hdf5\")\n","  print(\"Loaded \"+model_name+\" model\")\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"By4V6UyFP5Yn","colab_type":"text"},"source":["## モデル構築"]},{"cell_type":"markdown","metadata":{"id":"hTs-qh9JfZJ-","colab_type":"text"},"source":["### AlexNet (https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YUXbomS7FTQI","colab":{}},"source":["from keras.models import Sequential\n","from keras.layers import Conv2D, MaxPool2D, Activation, BatchNormalization, Flatten, Dense, Dropout\n","from keras.optimizers import SGD\n","from keras.initializers import RandomNormal\n","\n","def AlexNet():\n","  model = Sequential()\n","\n","  model.add(Conv2D(filters=96, kernel_size=(11, 11), strides=(4, 4), padding=\"same\", activation=\"relu\", kernel_initializer=RandomNormal(mean=0, stddev=0.01), bias_initializer=\"zeros\" ,input_shape=(224, 224, 1)))\n","  model.add(MaxPool2D(pool_size=(3, 3), strides=(2, 2)))\n","  model.add(BatchNormalization())\n","\n","  model.add(Conv2D(filters=256, kernel_size=(5, 5), padding=\"same\", activation=\"relu\", kernel_initializer=RandomNormal(mean=0, stddev=0.01), bias_initializer=\"ones\"))\n","  model.add(MaxPool2D(pool_size=(3, 3), strides=(2, 2)))\n","  model.add(BatchNormalization())\n","\n","  model.add(Conv2D(filters=384, kernel_size=(3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=RandomNormal(mean=0, stddev=0.01), bias_initializer=\"zeros\"))\n","  model.add(Conv2D(filters=384, kernel_size=(3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=RandomNormal(mean=0, stddev=0.01), bias_initializer=\"ones\"))\n","  model.add(Conv2D(filters=256, kernel_size=(3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=RandomNormal(mean=0, stddev=0.01), bias_initializer=\"ones\"))\n","  model.add(MaxPool2D(pool_size=(3, 3), strides=(2, 2)))\n","  model.add(BatchNormalization())\n","\n","  model.add(Flatten())\n","  model.add(Dense(units=4096, activation=\"relu\", kernel_initializer=RandomNormal(mean=0, stddev=0.01), bias_initializer=\"zeros\"))\n","  model.add(Dropout(0.5))\n","  model.add(Dense(units=4096, activation=\"relu\", kernel_initializer=RandomNormal(mean=0, stddev=0.01), bias_initializer=\"zeros\"))\n","  model.add(Dropout(0.5))\n","\n","  model.add(Dense(2, activation=\"softmax\"))\n","\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LQIfFe-AFSPe"},"source":["### VGG16 (https://arxiv.org/pdf/1409.1556.pdf)"]},{"cell_type":"code","metadata":{"id":"bVRC7iBipEZK","colab_type":"code","colab":{}},"source":["from keras.models import Sequential\n","from keras.layers import Conv2D, MaxPool2D, Activation, BatchNormalization, Flatten, Dense, Dropout\n","from keras.optimizers import SGD\n","from keras.initializers import RandomNormal\n","\n","def VGG16():\n","  model = Sequential()\n","\n","  model.add(Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=RandomNormal(mean=0, stddev=0.01),input_shape=(224, 224, 1)))\n","  model.add(Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=RandomNormal(mean=0, stddev=0.01)))\n","  model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n","  model.add(BatchNormalization())\n","\n","  model.add(Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=RandomNormal(mean=0, stddev=0.01)))\n","  model.add(Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=RandomNormal(mean=0, stddev=0.01)))\n","  model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n","  model.add(BatchNormalization())\n","\n","  model.add(Conv2D(filters=256, kernel_size=(3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=RandomNormal(mean=0, stddev=0.01)))\n","  model.add(Conv2D(filters=256, kernel_size=(3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=RandomNormal(mean=0, stddev=0.01)))\n","  model.add(Conv2D(filters=256, kernel_size=(3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=RandomNormal(mean=0, stddev=0.01)))\n","  model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n","  model.add(BatchNormalization())\n","\n","  model.add(Conv2D(filters=512, kernel_size=(3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=RandomNormal(mean=0, stddev=0.01)))\n","  model.add(Conv2D(filters=512, kernel_size=(3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=RandomNormal(mean=0, stddev=0.01)))\n","  model.add(Conv2D(filters=512, kernel_size=(3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=RandomNormal(mean=0, stddev=0.01)))\n","  model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n","  model.add(BatchNormalization())\n","\n","  model.add(Conv2D(filters=512, kernel_size=(3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=RandomNormal(mean=0, stddev=0.01)))\n","  model.add(Conv2D(filters=512, kernel_size=(3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=RandomNormal(mean=0, stddev=0.01)))\n","  model.add(Conv2D(filters=512, kernel_size=(3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=RandomNormal(mean=0, stddev=0.01)))\n","  model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n","  model.add(BatchNormalization())\n","\n","  model.add(Flatten())\n","  model.add(Dense(units=4096, activation=\"relu\", kernel_initializer=RandomNormal(mean=0, stddev=0.01), bias_initializer=\"zeros\"))\n","  model.add(Dropout(0.5))\n","  model.add(Dense(units=4096, activation=\"relu\", kernel_initializer=RandomNormal(mean=0, stddev=0.01), bias_initializer=\"zeros\"))\n","  model.add(Dropout(0.5))\n","\n","  model.add(Dense(2, activation=\"softmax\"))\n","\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9BnYYoXbGJTq","colab_type":"text"},"source":["### GoogLeNet (https://arxiv.org/pdf/1409.4842.pdf)"]},{"cell_type":"code","metadata":{"id":"Inq2Z07VVyDM","colab_type":"code","colab":{}},"source":["from keras.layers import Input, Dense, Conv2D, MaxPooling2D\n","from keras.models import Model\n","from keras.layers.merge import concatenate\n","from keras.regularizers import l2\n","from keras.layers import Input, Dense, Conv2D, MaxPooling2D, AveragePooling2D, Dropout, Flatten\n","from keras.optimizers import SGD\n","\n","\n","def inception_model(input, filters_1x1, filters_3x3_reduce, filters_3x3, filters_5x5_reduce, filters_5x5, filters_pool_proj):\n","  conv_1x1 = Conv2D(filters=filters_1x1, kernel_size=(1, 1), padding='same', activation='relu', kernel_regularizer=l2(0.01))(input)\n","  conv_3x3_reduce = Conv2D(filters=filters_3x3_reduce, kernel_size=(1, 1), padding='same', activation='relu', kernel_regularizer=l2(0.01))(input)\n","  conv_3x3 = Conv2D(filters=filters_3x3, kernel_size=(3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.01))(conv_3x3_reduce)\n","  conv_5x5_reduce  = Conv2D(filters=filters_5x5_reduce, kernel_size=(1, 1), padding='same', activation='relu', kernel_regularizer=l2(0.01))(input)\n","  conv_5x5 = Conv2D(filters=filters_5x5, kernel_size=(5, 5), padding='same', activation='relu', kernel_regularizer=l2(0.01))(conv_5x5_reduce)\n","  maxpool = MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(input)\n","  maxpool_proj = Conv2D(filters=filters_pool_proj, kernel_size=(1, 1), strides=(1, 1), padding='same', activation='relu', kernel_regularizer=l2(0.01))(maxpool)\n","  inception_output = concatenate([conv_1x1, conv_3x3, conv_5x5, maxpool_proj], axis=3)  # use tf as backend\n","  return inception_output\n","\n","def GoogLeNet():\n","  input = Input(shape=(224, 224, 1))\n","  conv1_7x7_s2 = Conv2D(filters=64, kernel_size=(7, 7), strides=(2, 2), padding='same', activation='relu', kernel_regularizer=l2(0.01))(input)\n","  maxpool1_3x3_s2 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(conv1_7x7_s2)\n","  conv2_3x3_reduce = Conv2D(filters=64, kernel_size=(1, 1), padding='same', activation='relu', kernel_regularizer=l2(0.01))(maxpool1_3x3_s2)\n","  conv2_3x3 = Conv2D(filters=192, kernel_size=(3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.01))(conv2_3x3_reduce)\n","  maxpool2_3x3_s2 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(conv2_3x3)\n","  inception_3a = inception_model(input=maxpool2_3x3_s2, filters_1x1=64, filters_3x3_reduce=96, filters_3x3=128, filters_5x5_reduce=16, filters_5x5=32, filters_pool_proj=32)\n","  inception_3b = inception_model(input=inception_3a, filters_1x1=128, filters_3x3_reduce=128, filters_3x3=192, filters_5x5_reduce=32, filters_5x5=96, filters_pool_proj=64)\n","  maxpool3_3x3_s2 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(inception_3b)\n","  inception_4a = inception_model(input=maxpool3_3x3_s2, filters_1x1=192, filters_3x3_reduce=96, filters_3x3=208, filters_5x5_reduce=16, filters_5x5=48, filters_pool_proj=64)\n","  inception_4b = inception_model(input=inception_4a, filters_1x1=160, filters_3x3_reduce=112, filters_3x3=224, filters_5x5_reduce=24, filters_5x5=64, filters_pool_proj=64)\n","  inception_4c = inception_model(input=inception_4b, filters_1x1=128, filters_3x3_reduce=128, filters_3x3=256, filters_5x5_reduce=24, filters_5x5=64, filters_pool_proj=64)\n","  inception_4d = inception_model(input=inception_4c, filters_1x1=112, filters_3x3_reduce=144, filters_3x3=288, filters_5x5_reduce=32, filters_5x5=64, filters_pool_proj=64)\n","  inception_4e = inception_model(input=inception_4d, filters_1x1=256, filters_3x3_reduce=160, filters_3x3=320, filters_5x5_reduce=32, filters_5x5=128, filters_pool_proj=128)\n","  maxpool4_3x3_s2 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(inception_4e)\n","  inception_5a = inception_model(input=maxpool4_3x3_s2, filters_1x1=256, filters_3x3_reduce=160, filters_3x3=320, filters_5x5_reduce=32, filters_5x5=128, filters_pool_proj=128)\n","  inception_5b = inception_model(input=inception_5a, filters_1x1=384, filters_3x3_reduce=192, filters_3x3=384, filters_5x5_reduce=48, filters_5x5=128, filters_pool_proj=128)\n","  averagepool1_7x7_s1 = AveragePooling2D(pool_size=(7, 7), strides=(7, 7), padding='same')(inception_5b)\n","  drop1 = Dropout(rate=0.4)(averagepool1_7x7_s1)\n","  linear = Dense(units=2, activation='softmax', kernel_regularizer=l2(0.01))(Flatten()(drop1))\n","  last = linear\n","\n","  model = Model(inputs=input, outputs=last)\n","\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nuXXoPl6oGBQ","colab_type":"text"},"source":["## 学習・保存・読込・評価"]},{"cell_type":"code","metadata":{"id":"04jiFvQQn-hF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"777f32cf-a8d9-4f90-ba14-897af2f29465","executionInfo":{"status":"error","timestamp":1586745456975,"user_tz":-540,"elapsed":54330,"user":{"displayName":"Shinnosuke Matsuo","photoUrl":"","userId":"04948413199740942872"}}},"source":["from keras.optimizers import SGD\n","from sklearn.metrics import confusion_matrix\n","\n","def model_learn(model_name, X_train, y_train_oh, X_val, y_val_oh, batch_size=128, epochs=90):\n","  model_dict = {\n","      \"AlexNet\": AlexNet(), \n","      \"VGG16\": VGG16(), \n","      \"GoogLeNet\": GoogLeNet()\n","  }\n","  if model_name in model_dict:\n","    model = model_dict[model_name]\n","  else:\n","    print(\"not model\")\n","  model.summary()\n","  model.compile(optimizer=SGD(lr=0.01, momentum=0.9, decay=0.0005), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n","  hist = model.fit(X_train, y_train_oh, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val_oh))\n","  plot_history_loss(hist, model_name)\n","  plot_history_acc(hist, model_name)\n","  model_save(model, model_name)\n","\n","\n","\n","# model learn pattern\n","model_learn(\"AlexNet\", X_train, y_train_oh, X_val, y_val_oh)\n","#model_learn(\"VGG16\", X_train, y_train_oh, X_val, y_val_oh)\n","#model_learn(\"GoogLeNet\", X_train, y_train_oh, X_val, y_val_oh)\n","\n","# model load pattern\n","#model = model_load(\"AlexNet\")\n","#model.compile(optimizer=SGD(lr=0.01, momentum=0.9, decay=0.0005), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n","\n","\n","\n","model_evaluate(model, X_test, y_test_oh)\n","pred = model.predict_classes(X_test)\n","print(pred)\n","print(y_test)\n","cm = confusion_matrix(y_test, pred)\n","print(cm)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Model: \"sequential_8\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_231 (Conv2D)          (None, 56, 56, 96)        11712     \n","_________________________________________________________________\n","max_pooling2d_67 (MaxPooling (None, 27, 27, 96)        0         \n","_________________________________________________________________\n","batch_normalization_28 (Batc (None, 27, 27, 96)        384       \n","_________________________________________________________________\n","conv2d_232 (Conv2D)          (None, 27, 27, 256)       614656    \n","_________________________________________________________________\n","max_pooling2d_68 (MaxPooling (None, 13, 13, 256)       0         \n","_________________________________________________________________\n","batch_normalization_29 (Batc (None, 13, 13, 256)       1024      \n","_________________________________________________________________\n","conv2d_233 (Conv2D)          (None, 13, 13, 384)       885120    \n","_________________________________________________________________\n","conv2d_234 (Conv2D)          (None, 13, 13, 384)       1327488   \n","_________________________________________________________________\n","conv2d_235 (Conv2D)          (None, 13, 13, 256)       884992    \n","_________________________________________________________________\n","max_pooling2d_69 (MaxPooling (None, 6, 6, 256)         0         \n","_________________________________________________________________\n","batch_normalization_30 (Batc (None, 6, 6, 256)         1024      \n","_________________________________________________________________\n","flatten_11 (Flatten)         (None, 9216)              0         \n","_________________________________________________________________\n","dense_25 (Dense)             (None, 4096)              37752832  \n","_________________________________________________________________\n","dropout_18 (Dropout)         (None, 4096)              0         \n","_________________________________________________________________\n","dense_26 (Dense)             (None, 4096)              16781312  \n","_________________________________________________________________\n","dropout_19 (Dropout)         (None, 4096)              0         \n","_________________________________________________________________\n","dense_27 (Dense)             (None, 2)                 8194      \n","=================================================================\n","Total params: 58,268,738\n","Trainable params: 58,267,522\n","Non-trainable params: 1,216\n","_________________________________________________________________\n","Train on 8966 samples, validate on 1448 samples\n","Epoch 1/90\n","8966/8966 [==============================] - 6s 717us/step - loss: 0.5706 - accuracy: 0.7892 - val_loss: 4.3288 - val_accuracy: 0.4793\n","Epoch 2/90\n","8966/8966 [==============================] - 6s 660us/step - loss: 0.3624 - accuracy: 0.8459 - val_loss: 4.0114 - val_accuracy: 0.4793\n","Epoch 3/90\n","8966/8966 [==============================] - 6s 664us/step - loss: 0.3336 - accuracy: 0.8606 - val_loss: 0.4637 - val_accuracy: 0.8550\n","Epoch 4/90\n","8966/8966 [==============================] - 6s 657us/step - loss: 0.2161 - accuracy: 0.9230 - val_loss: 0.4444 - val_accuracy: 0.8329\n","Epoch 5/90\n","8966/8966 [==============================] - 6s 663us/step - loss: 0.1568 - accuracy: 0.9499 - val_loss: 0.2533 - val_accuracy: 0.9151\n","Epoch 6/90\n","8966/8966 [==============================] - 6s 660us/step - loss: 0.1893 - accuracy: 0.9343 - val_loss: 2.4557 - val_accuracy: 0.5974\n","Epoch 7/90\n","8966/8966 [==============================] - 6s 661us/step - loss: 0.1804 - accuracy: 0.9398 - val_loss: 1.5617 - val_accuracy: 0.5981\n","Epoch 8/90\n","8966/8966 [==============================] - 6s 663us/step - loss: 0.1335 - accuracy: 0.9590 - val_loss: 2.7708 - val_accuracy: 0.5256\n","Epoch 9/90\n","5760/8966 [==================>...........] - ETA: 1s - loss: 0.1099 - accuracy: 0.9679"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-633e29d3d4cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mmodel_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmodel_learn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AlexNet\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_oh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_oh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-21-633e29d3d4cd>\u001b[0m in \u001b[0;36mmodel_learn\u001b[0;34m(model_name, X_train, y_train_oh, X_val, y_val_oh, batch_size, epochs)\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_oh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_oh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m   \u001b[0mplot_history_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mplot_history_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}